---
title: "Latent Concept Disentanglement in Transformer-based Language Models"
collection: publications
category: accepted
permalink: https://openreview.net/forum?id=k3SEVOW2Dg
excerpt: ''
date: 2025-09-25
venue: 'ICLR 2026'
#slidesurl: ''
paperurl: 'https://www.arxiv.org/abs/2506.16975'
authors: 'Guanzhe Hong<sup>*</sup>, Bhavya Vasudeva<sup>*</sup>, Vatsal Sharan, Cyrus Rashtchian, Prabhakar Raghavan, Rina Panigrahy'
#citation: 'Bhavya Vasudeva, Jung Whan Lee, Vatsal Sharan, Mahdi Soltanolkotabi. &quot;The Rich and the Simple: On the Implicit Bias of Adam and SGD.&quot; <i> Under Review</i>.'
---

When large language models (LLMs) use in-context learning (ICL) to solve a new task, they must infer latent concepts from demonstration examples. This raises the question of whether and how transformers represent latent structures as part of their computation. Our work experiments with several controlled tasks, studying this question using mechanistic interpretability. First, we show that in transitive reasoning tasks with a latent, discrete concept, the model successfully identifies the latent concept and does step-by-step concept composition. This builds upon prior work that analyzes single-step reasoning. Then, we consider tasks parameterized by a latent numerical concept. We discover low-dimensional subspaces in the model's representation space, where the geometry cleanly reflects the underlying parameterization. Overall, we show that small and large models can indeed disentangle and utilize latent concepts that they learn in-context from a handful of abbreviated demonstrations.