---
title: "Transformers Learn Low-Sensitivity Functions: Investigations and Implications"
collection: publications
category: accepted
permalink: https://openreview.net/forum?id=4ikjWBs3tE
excerpt: ''
date: 2024-03-01
venue: 'ICLR 2025'
paperurl: 'https://arxiv.org/abs/2403.06925.pdf'
posterurl: 'https://drive.google.com/file/d/18_dAucZdIL2X65Tr02I7-QzNGZq_wHMX/view?usp=share_link'
authors: 'Bhavya Vasudeva<sup>*</sup>, Deqing Fu<sup>*</sup>, Tianyi Zhou, Elliott Kau, Youqi Huang, Vatsal Sharan'
#citation: 'Bhavya Vasudeva<sup>*</sup>, Deqing Fu<sup>*</sup>, Tianyi Zhou, Elliott Kau, Youqi Huang, Vatsal Sharan. &quot;Transformers Learn Low-Sensitivity Functions: Investigations and Implications.&quot; <i>ICLR 2025</i>.'
---

Transformers achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of their inductive biases and how those biases differ from other neural network architectures remains elusive. In this work, we identify the sensitivity of the model to token-wise random perturbations in the input as a unified metric which explains the inductive bias of transformers across different data modalities and distinguishes them from other architectures. We show that transformers have lower sensitivity than MLPs, CNNs, ConvMixers and LSTMs, across both vision and language tasks. We also show that this low-sensitivity bias has important implications: i) lower sensitivity correlates with improved robustness; it can also be used as an efficient intervention to further improve the robustness of transformers; ii) it corresponds to flatter minima in the loss landscape; and iii) it can serve as a progress measure for grokking. We support these findings with theoretical results showing (weak) spectral bias of transformers in the NTK regime, and improved robustness due to the lower sensitivity.
