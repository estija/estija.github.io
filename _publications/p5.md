---
title: "In-Context Occam’s Razor: How Transformers Prefer Simpler Hypotheses on the Fly"
collection: publications
category: preprint
#permalink: 
excerpt: ''
date: 2025-03-25
venue: 'Submitted'
#slidesurl: ''
paperurl: ''
authors: 'Puneesh Deora, Bhavya Vasudeva, Tina Behnia, Christos Thrampoulidis'
#citation: 'Bhavya Vasudeva, Jung Whan Lee, Vatsal Sharan, Mahdi Soltanolkotabi. &quot;The Rich and the Simple: On the Implicit Bias of Adam and SGD.&quot; <i> Under Review</i>.'
---

In-context learning (ICL) enables transformers to adapt to new tasks through contextual examples without parameter updates. While existing research has typically studied ICL in fixed-complexity environments, real-world language models encounter tasks spanning diverse complexity levels. This paper investigates how transformers navigate hierarchical task structures where higher-complexity categories can perfectly represent any pattern generated by simpler ones. We design testbeds based on both Markov chains and linear regression that reveal transformers not only identify the appropriate complexity level for each task but also accurately infer the corresponding parameters—even when the in-context examples are compatible with multiple complexity hypotheses. Notably, when presented with data generated by simpler processes, transformers consistently favor the least complex sufficient explanation. We theoretically explain this behavior through a Bayesian framework, demonstrating that transformers effectively implement an in-context Bayesian Occam's razor by balancing model fit against complexity penalties.